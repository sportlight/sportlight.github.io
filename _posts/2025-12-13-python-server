---
title: "[Side Project] ë°ì´í„° ìœ ì¶œ ì—†ëŠ” ì˜¨í”„ë ˆë¯¸ìŠ¤ AI ì—ì´ì „íŠ¸ êµ¬ì¶• (1) - FastAPIì™€ Vector DB ì—°ë™"
date: 2025-12-13 13:00:00 +0900
categories: [AI-Engineering, Backend]
tags: [python, fastapi, rag, vectordb, chromadb, langchain, on-premise, llm, llama3]
mermaid: true
---

## 1. í”„ë¡œì íŠ¸ ê°œìš” (Overview)

SaaSí˜• AI(ChatGPT, Claude)ëŠ” ê°•ë ¥í•˜ì§€ë§Œ, ê¸°ì—…ì˜ ë¯¼ê° ë°ì´í„°ë‚˜ ê°œì¸ì •ë³´ë¥¼ ì™¸ë¶€ ì„œë²„ë¡œ ì „ì†¡í•´ì•¼ í•œë‹¤ëŠ” ë³´ì•ˆ ì´ìŠˆê°€ ì¡´ì¬í•œë‹¤.
ì´ë²ˆ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ì˜ ëª©í‘œëŠ” **"ì¸í„°ë„· ì—°ê²° ì—†ì´ ë¡œì»¬ í™˜ê²½(On-Premise)ì—ì„œ ë™ì‘í•˜ëŠ” ë‚˜ë§Œì˜ AI ì—ì´ì „íŠ¸"**ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ë‹¤.

í”íˆ ë°ì´í„°ë¥¼ ë„£ì–´ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì„ íŒŒì¸íŠœë‹(Fine-tuning)ì´ë¼ ë¶€ë¥´ì§€ë§Œ, ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ê°€ ìš©ì´í•˜ê³  ë¹„ìš©ì´ ì €ë ´í•œ **RAG(Retrieval-Augmented Generation, ê²€ìƒ‰ ì¦ê°• ìƒì„±)** ë°©ì‹ì„ ì±„íƒí•œë‹¤. ì¦‰, ë²¡í„° DB(Vector DB)ë¥¼ ë‘ë‡Œì˜ ì¥ê¸° ê¸°ì–µì¥ì¹˜ë¡œ í™œìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤.

### ğŸ›  Tech Stack
* **Language:** Python 3.10+
* **Server Framework:** FastAPI (ë¹„ë™ê¸° ì²˜ë¦¬ ë° API ë¬¸ì„œí™” ìš©ì´)
* **Orchestration:** LangChain (LLMê³¼ DB ì—°ê²° íŒŒì´í”„ë¼ì¸)
* **Vector DB:** ChromaDB (ë¡œì»¬ íŒŒì¼ ê¸°ë°˜, ê°€ë³ê³  ë¹ ë¦„)
* **LLM Engine:** Llama.cpp (CPU/GPU ê²¸ìš© ê²½ëŸ‰í™” ëª¨ë¸ êµ¬ë™)
* **Model:** Llama-3-8B-Quantized (GGUF í¬ë§· ì‚¬ìš©)

---

## 2. ì•„í‚¤í…ì²˜ ì„¤ê³„ (Architecture)

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ë²¡í„° DBì—ì„œ ê´€ë ¨ ì§€ì‹ì„ ê²€ìƒ‰(Retrieval)í•˜ê³ , ì´ë¥¼ LLMì— í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì…í•˜ì—¬ ë‹µë³€ì„ ìƒì„±(Generation)í•˜ëŠ” íë¦„ì´ë‹¤.

```mermaid
flowchart LR
    User[Client] -->|POST /chat| API[FastAPI Server]
    
    subgraph On_Premise_Server
        direction TB
        API -->|1. Query Embedding| Embed[Local Embedding Model]
        Embed -->|2. Similarity Search| VDB[(Chroma Vector DB)]
        VDB -->|3. Retrieve Context| API
        
        API -->|4. Prompt + Context| LLM[Local LLM (Llama-3)]
        LLM -->|5. Generate Answer| API
    end
    
    API -->|6. JSON Response| User
```

---

## 3. ê°œë°œ í™˜ê²½ ì„¤ì • (Setup)

ì™¸ë¶€ APIë¥¼ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë¡œì»¬ì—ì„œ êµ¬ë™ ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•´ì•¼ í•œë‹¤.

### 3.1. ê°€ìƒí™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# llama-cpp-pythonì€ í•˜ë“œì›¨ì–´ ê°€ì†(Metal/CUDA) ì§€ì›ì„ ìœ„í•´ ë³„ë„ ë¹Œë“œê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
pip install fastapi uvicorn \
            langchain langchain-community langchain-huggingface \
            chromadb \
            llama-cpp-python \
            python-multipart \
            pydantic
```

### 3.2. ëª¨ë¸ íŒŒì¼ ì¤€ë¹„ (GGUF)
ë¡œì»¬ì—ì„œ ëŒë¦¬ê¸° ìœ„í•´ ì–‘ìí™”ëœ(Quantized) ëª¨ë¸ì´ í•„ìš”í•˜ë‹¤. HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í”„ë¡œì íŠ¸ í´ë” ë‚´ `models/` ë””ë ‰í† ë¦¬ì— ìœ„ì¹˜ì‹œí‚¨ë‹¤.
* **ì¶”ì²œ ëª¨ë¸:** `Meta-Llama-3-8B-Instruct.Q4_K_M.gguf` (ì•½ 4GB~5GB)

---

## 4. í•µì‹¬ ëª¨ë“ˆ êµ¬í˜„

### 4.1. ë²¡í„° DB ë§¤ë‹ˆì € (RAG Core)
ë¬¸ì„œ(Text)ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•˜ê³ , ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆì´ë‹¤. ì„ë² ë”© ëª¨ë¸ ë˜í•œ ì™¸ë¶€ APIê°€ ì•„ë‹Œ ë¡œì»¬ ëª¨ë¸(`all-MiniLM-L6-v2`)ì„ ì‚¬ìš©í•œë‹¤.

```python
# app/rag_core.py
import os
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

class VectorDBManager:
    def __init__(self, db_path="./chroma_db"):
        # 1. ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (HuggingFace)
        # CPU í™˜ê²½ì—ì„œë„ ë¹ ë¥´ê³  ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©
        self.embedding_func = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'} # GPUê°€ ìˆë‹¤ë©´ 'cuda'
        )
        self.db_path = db_path
        
        # 2. ChromaDB ë¡œë“œ (Persist)
        # ë””ìŠ¤í¬ì— ì €ì¥í•˜ì—¬ ì„œë²„ ì¬ì‹œì‘ í›„ì—ë„ ë°ì´í„° ìœ ì§€
        self.vectordb = Chroma(
            persist_directory=self.db_path,
            embedding_function=self.embedding_func
        )

    def add_documents(self, text_list: list[str]):
        """
        í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì²­í¬(Chunk)ë¡œ ë‚˜ëˆ„ì–´ ë²¡í„° DBì— ì €ì¥
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,     # 500ì ë‹¨ìœ„ë¡œ ìë¦„
            chunk_overlap=50    # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ 50ì ì¤‘ë³µ
        )
        docs = [Document(page_content=t) for t in text_list]
        splits = text_splitter.split_documents(docs)
        
        self.vectordb.add_documents(splits)
        print(f"âœ… {len(splits)} chunks added to Vector DB.")

    def get_retriever(self):
        """LangChain Chain ì—°ê²°ì„ ìœ„í•œ Retriever ë°˜í™˜"""
        # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ 3ê°œ ë¬¸ì„œ ì°¸ì¡°
        return self.vectordb.as_retriever(search_kwargs={"k": 3})
```

### 4.2. LLM ì—”ì§„ (Local Model Loader)
`llama-cpp-python`ì„ ì‚¬ìš©í•˜ì—¬ GGUF ëª¨ë¸ì„ ë¡œë“œí•œë‹¤. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ì— ë”°ë¼ `n_gpu_layers` ë“±ì˜ íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìš”í•˜ë‹¤.

```python
# app/llm_engine.py
import os
from langchain_community.llms import LlamaCpp
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

def load_llm(model_path="./models/llama-3-8b.gguf"):
    """
    ë¡œì»¬ Llama-3 ëª¨ë¸ ë¡œë“œ
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model not found at {model_path}")

    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
    
    llm = LlamaCpp(
        model_path=model_path,
        temperature=0.1,    # RAGì—ì„œëŠ” í™˜ê°(Hallucination) ë°©ì§€ë¥¼ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
        n_ctx=4096,         # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° (ì…ë ¥ + ì¶œë ¥ í† í° í•œê³„)
        max_tokens=1024,    # ë‹µë³€ ìµœëŒ€ ê¸¸ì´
        n_gpu_layers=0,     # GPU ì‚¬ìš© ì‹œ ë ˆì´ì–´ ìˆ˜ ì¡°ì ˆ (Mac Metal: -1)
        callback_manager=callback_manager,
        verbose=True
    )
    return llm
```

### 4.3. FastAPI ë©”ì¸ ì„œë²„ (Main)
LangChainì˜ `RetrievalQA` ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ DB ê²€ìƒ‰ ê²°ê³¼ì™€ LLM ì¶”ë¡ ì„ ì—°ê²°í•œë‹¤.

```python
# main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from app.rag_core import VectorDBManager
from app.llm_engine import load_llm
from langchain.chains import RetrievalQA

app = FastAPI(title="On-Premise AI Agent Server")

# --- ì „ì—­ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ---
print("Initializing Vector DB...")
vdb_manager = VectorDBManager()

print("Loading Local LLM...")
llm = load_llm()

# --- RAG ì²´ì¸ ìƒì„± ---
# retrieverê°€ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ì˜¤ë©´, llmì´ ê·¸ê²ƒì„ ë³´ê³  ë‹µë³€ì„ ìƒì„±í•¨
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vdb_manager.get_retriever(),
    return_source_documents=True # ë‹µë³€ì˜ ì¶œì²˜ í™•ì¸ìš©
)

# --- DTO ì •ì˜ ---
class QueryRequest(BaseModel):
    query: str

class DocRequest(BaseModel):
    texts: list[str]

# --- API Endpoints ---

@app.get("/")
def health_check():
    return {"status": "ok", "mode": "on-premise"}

@app.post("/ingest")
async def ingest_knowledge(request: DocRequest):
    """
    [í•™ìŠµ ë‹¨ê³„] í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì£¼ì… (Embedding & Indexing)
    """
    try:
        vdb_manager.add_documents(request.texts)
        return {"status": "success", "message": f"{len(request.texts)} documents ingested."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat")
async def chat(request: QueryRequest):
    """
    [ì¶”ë¡  ë‹¨ê³„] RAG ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€
    """
    try:
        # LangChain invoke
        result = qa_chain.invoke({"query": request.query})
        
        # ì¶œì²˜ ë¬¸ì„œ ì •ë¦¬
        sources = [doc.page_content for doc in result['source_documents']]
        
        return {
            "query": request.query,
            "answer": result['result'],
            "context_used": sources
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 5. ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸ (Testing)

ì„œë²„ë¥¼ ì‹¤í–‰í•˜ê³ , ëª¨ë¸ì´ ë‚´ê°€ ì£¼ì…í•œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤.

```bash
# ì„œë²„ ì‹¤í–‰
python main.py
```

### 5.1. ì§€ì‹ ì£¼ì… (Ingest)
ì¼ë°˜ì ì¸ LLMì€ ì•Œ ìˆ˜ ì—†ëŠ”, ë‚˜ë§Œì˜ 'ì‚¬ì´ë“œ í”„ë¡œì íŠ¸' ì •ë³´ë¥¼ ì£¼ì…í•´ ë³¸ë‹¤.

```bash
curl -X POST "http://localhost:8000/ingest" \
     -H "Content-Type: application/json" \
     -d '{
           "texts": [
             "2025ë…„ ì§„í–‰í•˜ëŠ” ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ì˜ ì½”ë“œëª…ì€ Project Zeroì…ë‹ˆë‹¤.",
             "Project Zeroì˜ í•µì‹¬ ëª©í‘œëŠ” ì˜¨í”„ë ˆë¯¸ìŠ¤ í™˜ê²½ì—ì„œ ì™„ë²½í•œ ë°ì´í„° ë³´ì•ˆì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
           ]
         }'
```

### 5.2. ì§ˆì˜ì‘ë‹µ (Chat)
ì£¼ì…ëœ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì„ ë˜ì§„ë‹¤.

```bash
curl -X POST "http://localhost:8000/chat" \
     -H "Content-Type: application/json" \
     -d '{"query": "Project Zeroì˜ ëª©í‘œê°€ ë­ì•¼?"}'
```

**[ì˜ˆìƒ ì‘ë‹µ]**
```json
{
  "query": "Project Zeroì˜ ëª©í‘œê°€ ë­ì•¼?",
  "answer": "Project Zeroì˜ í•µì‹¬ ëª©í‘œëŠ” ì˜¨í”„ë ˆë¯¸ìŠ¤ í™˜ê²½ì—ì„œ ì™„ë²½í•œ ë°ì´í„° ë³´ì•ˆì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.",
  "context_used": ["Project Zeroì˜ í•µì‹¬ ëª©í‘œëŠ”..."]
}
```

---

## 6. ê²°ë¡  (Conclusion)

ì´ë¡œì¨ ì™¸ë¶€ ì¸í„°ë„· ì—°ê²° ì—†ì´ ë™ì‘í•˜ëŠ” **Private AI ì„œë²„**ì˜ ê¸°ì´ˆë¥¼ ë‹¤ì¡Œë‹¤.
ì¼ë°˜ì ì¸ íŒŒì¸íŠœë‹(Fine-tuning)ì€ GPU ìì›ì´ ë§ì´ ë“¤ê³  ëª¨ë¸ ì¬í•™ìŠµ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ, **Vector DBë¥¼ í™œìš©í•œ RAG ë°©ì‹**ì€ ë¬¸ì„œë¥¼ ë„£ëŠ” ì¦‰ì‹œ AIê°€ ì§€ì‹ì„ ìŠµë“í•œë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.

### Next Steps
1.  **PDF/Word íŒŒì„œ ì—°ë™:** ë‹¨ìˆœ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ íŒŒì¼ ì—…ë¡œë“œ ê¸°ëŠ¥ êµ¬í˜„
2.  **GPU ê°€ì† ìµœì í™”:** `n_gpu_layers` íŠœë‹ì„ í†µí•œ ì‘ë‹µ ì†ë„ ê°œì„ 
3.  **Dockerizing:** ë°°í¬ í¸ì˜ì„±ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë¹Œë“œ
